{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apriori.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkiJKJf9CnJU",
        "colab_type": "code",
        "outputId": "0efee42c-c4c4-475e-e6a8-881cbd4a2f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "pip install mlxtend"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.6/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.17.5)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (3.1.3)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.25.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend) (45.1.0)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.22.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->mlxtend) (0.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->mlxtend) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMbDfpd7LoJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import os\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3gfPJ2TXhE9",
        "colab_type": "code",
        "outputId": "9e25cb5d-1ee0-41ca-9844-16fa88ec413c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'enron_3_0.02.csv',\n",
              " 'nips_3_0.5.csv',\n",
              " 'document_nips.csv',\n",
              " 'document_enron.csv',\n",
              " 'kos_2_0.2.csv',\n",
              " 'kos_2_0.2',\n",
              " 'document_kos.csv',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR2TvsbADFeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH='' # Directory for all txt files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS3Z1_L5DgfG",
        "colab_type": "text"
      },
      "source": [
        "# $Question$ $1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdAhbHbTLx4p",
        "colab_type": "text"
      },
      "source": [
        "## Creating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE4fItMwNTJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DatasetGeneration(word_directory,vocab_directory):\n",
        "  df=pd.read_csv(word_directory,header=None)\n",
        "\n",
        "  # Dropping first three values\n",
        "  d,w,nnz,df=(df.iloc[0],df.iloc[0],df.iloc[0],df.drop([0,1,2],axis=0))\n",
        "\n",
        "  # Reindexing\n",
        "  df.index=range(len(df))\n",
        "\n",
        "  # Loading Vocabulary dataset\n",
        "  df_vocab=pd.read_csv(vocab_directory,header=None)\n",
        "\n",
        "  df_vocab['wordid']=list(range(1,len(df_vocab)+1))\n",
        "\n",
        "\n",
        "  df['docid']=df[0].apply(lambda x: int(str(x).split()[0]))\n",
        "  df['wordid']=df[0].apply(lambda x: int(str(x).split()[1]))\n",
        "  df['count']=df[0].apply(lambda x: int(str(x).split()[2]))\n",
        "  df=df.drop(0,axis=1)\n",
        "\n",
        "\n",
        "  data=df.merge(df_vocab,on='wordid',how='left')\n",
        "  data.columns=list(data.columns[:-1])+['words']\n",
        "\n",
        "  #data.to_csv(f\"document_{word_directory.split('.')[1]}.csv\",index=False)  \n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUnUSO2yOOXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enron_words=DatasetGeneration(PATH+'docword.enron.txt',PATH+'vocab.enron.txt')\n",
        "nips_words=DatasetGeneration(PATH+'docword.nips.txt',PATH+'vocab.nips.txt')\n",
        "kos_words=DatasetGeneration(PATH+'docword.kos.txt',PATH+'vocab.kos.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5nxf_xDfqyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kos_words=pd.read_csv('document_kos.csv')\n",
        "nips_words=pd.read_csv('document_nips.csv')\n",
        "enron_words=pd.read_csv('document_enron.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzr7li1sDNoR",
        "colab_type": "text"
      },
      "source": [
        "# Imputing Null Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QbDDAo01oj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enron_words[enron_words['words'].isna()]='null'\n",
        "nips_words[nips_words['words'].isna()]='null'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBP1pY6TDTg9",
        "colab_type": "text"
      },
      "source": [
        "# Deleting all those words which is present in only one transaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gyr5ORpzt3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=nips_words.groupby(['words']).count()\n",
        "words=list(x[x['count']>1].index)\n",
        "nips_words=nips_words[nips_words['words'].isin(words)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1cYm1lQ5oD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=enron_words.groupby(['words']).count()\n",
        "words=list(x[x['count']>1].index)\n",
        "enron_words=enron_words[enron_words['words'].isin(words)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGcGkkaVOz-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=kos_words.groupby(['words']).count()\n",
        "words=list(x[x['count']>1].index)\n",
        "kos_words=kos_words[kos_words['words'].isin(words)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo8Rgw91DbNv",
        "colab_type": "text"
      },
      "source": [
        "# Creating Transaction Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-bezFFWTyOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kos_doc=list(kos_words.groupby(['docid'])['words'].apply(lambda x: list(x.unique())))\n",
        "nips_doc=list(nips_words.groupby(['docid'])['words'].apply(lambda x: list(x.unique())))\n",
        "enron_doc=list(enron_words.groupby(['docid'])['words'].apply(lambda x: list(x.unique())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGF7qwZfDx2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def freqItemset(transaction,min_sup,itemset):\n",
        "  te=TransactionEncoder()\n",
        "  te_ary=te.fit(transaction).transform(transaction,sparse=True)\n",
        "  document=pd.DataFrame.sparse.from_spmatrix(te_ary,columns=te.columns_)\n",
        "\n",
        "  frequent=mlxtend.frequent_patterns.apriori(document,min_support=min_sup,use_colnames=True)\n",
        "  frequent_itemsets = apriori(document, min_support=min_sup, use_colnames=True)\n",
        "  frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
        "  return frequent_itemsets[frequent_itemsets['length']==itemset]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zv35Bg3uB7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 15 min 37s for min_sup=0.5 and itemset=3 # Nips dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwYMSaH3mamV",
        "colab_type": "text"
      },
      "source": [
        "# $Question$ $2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L24rJAmgmUtA",
        "colab_type": "text"
      },
      "source": [
        "## Multiple Minimum Support "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbhwmiaqmHBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class multiple_minimum_support:\n",
        "\n",
        "  \"\"\" Custom Class for MS-Apriori Algorithm\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,dataset,alpha,lmbda,k,phi):\n",
        "    self.dataset=dataset\n",
        "    self.transactions=self.dataset['docid'].nunique() # No of transactions\n",
        "    self.dataset_doc=self.dataset.groupby(['docid'])['words'].apply(lambda x: list(x.unique()))\n",
        "    self.lmbda=lmbda # Parameter\n",
        "    self.alpha=alpha\n",
        "    self.k=k # integer k for k-itemset generation\n",
        "    self.phi=phi # upper bound for maximum support difference\n",
        "\n",
        "    self.vocab=self.dataset['words'].unique()\n",
        "    \n",
        "    self.MS=dict() # Stores all MIS values\n",
        "\n",
        "    self.support=dict(self.dataset.groupby(['words'])['docid'].count()/self.transactions)# DIctionary of all support counts\n",
        "\n",
        "    \"\"\"for i in self.vocab:\n",
        "      sup=self.dataset[self.dataset['words']==o]['docid'].nunique()/self.transactions\n",
        "      self.support[i]=sup\"\"\"\n",
        "    # Sorting ALl the support\n",
        "    self.MS_sorted=sorted(self.support,key=self.support.get)\n",
        "    \n",
        "\n",
        "    for i in self.MS_sorted:\n",
        "      self.MS[i]=self.lmbda*self.support[i]\n",
        "    \n",
        "    del self.MS_sorted\n",
        "\n",
        "    self.freqDataset=pd.DataFrame({'Item':[],'Support':[],'MIS':[],'Length':[]})\n",
        "\n",
        "    \n",
        "  \n",
        "  def count(self,x):\n",
        "    \"\"\" \n",
        "    Support Count for itemset x\n",
        "\n",
        "    \"\"\"\n",
        "    df=self.dataset[self.dataset['words'].isin(x)]\n",
        "    docids=df.groupby(['docid']).count()\n",
        "    docids=docids[docids['count']==len(x)]\n",
        "\n",
        "    return len(docids)/self.transactions\n",
        "\n",
        "  def sort(self,x):\n",
        "\n",
        "    \"\"\" sort values according to the mis values\n",
        "    \"\"\"\n",
        "    if self.MS[x[-2]]>self.MS[x[-1]]:\n",
        "      x[-2],x[-1]=x[-1],x[-2]\n",
        "    return x\n",
        "\n",
        "\n",
        "  def ms_apriori(self):\n",
        "\n",
        "\n",
        "    F1=[[i] for i in self.vocab if self.support[i]>=self.MS[i] and self.support[i]>self.alpha]\n",
        "    S1=[self.support[i[0]] for i in F1]\n",
        "    MS1=[self.MS[i[0]] for i in F1]\n",
        "    L1=[1]*len(F1)\n",
        "\n",
        "    temp=pd.DataFrame({'Item':F1,'Support':S1,'MIS':MS1,'Length':L1})\n",
        "    self.freqDataset=pd.concat([self.freqDataset,temp],axis=0)\n",
        "\n",
        "    del temp,S1,MS1,L1\n",
        "\n",
        "    for k in range(2,self.k+1):\n",
        "\n",
        "      if k==2:\n",
        "        C2=self.level2_candidate_generation()\n",
        "\n",
        "        F2=[i for i in C2 if self.count(i)>=self.MS[i[0]]]\n",
        "        S2=[self.count(i) for i in F2]\n",
        "        MS2=[self.MS[i[0]] for i in F2]\n",
        "        L2=[2]*len(F2)\n",
        "\n",
        "        temp=pd.DataFrame({'Item':F2,'Support':S2,'MIS':MS2,'Length':L2})\n",
        "        self.freqDataset=pd.concat([self.freqDataset,temp],axis=0)\n",
        "        del temp,S2,MS2,L2\n",
        "      else:\n",
        "        C=self.MScancidate_gen(self.freqDataset[self.freqDataset['Length']==(k-1)]['Item'])\n",
        "        F=[self.sort(i) for i in C if self.count(i)>=self.MS[i[0]]]\n",
        "        S=[self.count(i) for i in F]\n",
        "        MS=[self.MS[i[0]] for i in F]\n",
        "        L=[k]*(len(F))\n",
        "\n",
        "        temp=pd.DataFrame({'Item':F,'Support':S,'MIS':MS,'Length':L})\n",
        "        self.freqDataset=pd.concat([self.freqDataset,temp],axis=0)\n",
        "\n",
        "        del temp,S,MS,L\n",
        "  \n",
        "  \n",
        "  \n",
        "  def level2_candidate_generation(self):\n",
        "    C2=[]\n",
        "    vocab=self.freqDataset[['Item','MIS']]\n",
        "    vocab=vocab.sort_values(by='MIS')\n",
        "    vocab=vocab['Item'].apply(lambda x: x[0])\n",
        "\n",
        "    for i,l in enumerate(vocab):\n",
        "      if self.support[l]>=self.MS[l]:\n",
        "        for h in vocab[i+1:]:\n",
        "          if self.support[h]>=self.MS[l] and abs(self.support[h]-self.support[l])<=self.phi:\n",
        "            C2.append([l,h])\n",
        "    return C2\n",
        "\n",
        "  def MScancidate_gen(self,Fk):\n",
        "    CK=[]\n",
        "    for i,item_1 in enumerate(Fk):\n",
        "      for j,item_2 in enumerate(Fk[i+1:]):\n",
        "        if item_1[:-1]==item_2[:-2]:\n",
        "          x=self.freqDataset[self.freqDataset['Item'].isin(item_1)]['Support']\n",
        "          y=self.freqDataset[self.freqDataset['Item'].isin(item_2)]['Support']\n",
        "          if abs(x-y)<=self.phi:\n",
        "            c=item_1.copy()\n",
        "            c.append(item_2[-1])\n",
        "\n",
        "            if self.subset_chcek(c,Fk):\n",
        "              CK.append(c)\n",
        "    return CK\n",
        "\n",
        "\n",
        "  def subset_chcek(self,x,y):\n",
        "    \"\"\" x <- Candidate set\n",
        "        y <- Dataset\n",
        "    \"\"\"\n",
        "    not_ok=0\n",
        "    y=list(y)\n",
        "\n",
        "    for j in range(len(x)):\n",
        "      for k in range(j,len(x)):\n",
        "        if j==0 or self.MS[x[1]]==self.MS[x[0]]:\n",
        "          if [x[j],x[k]] not in y:\n",
        "            not_ok+=1\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"Frequent Itemset Generation Using Multiple Minimum Support\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcKRYfDQTXPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mis=multiple_minimum_support(kos_words,0.2,0.2,k=2,phi=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1zmX7iZTZ9F",
        "colab_type": "code",
        "outputId": "984c3694-ef6b-4268-f165-5f1ef43120f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "mis.ms_apriori()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 15.5 s, sys: 44.6 ms, total: 15.5 s\n",
            "Wall time: 15.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9jlYkUmsWIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}